{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from model import DS_out, device, dict_args, init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "from monai.losses.dice import GeneralizedDiceLoss\n",
    "\n",
    "loss_fn = BCELoss()\n",
    "dic_loss_fn = GeneralizedDiceLoss(to_onehot_y=True, softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FCT_Tail(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        filters = [32, 16, 8] \n",
    "        # number of blocks used in the model\n",
    "\n",
    "        # self.ds7 = DS_out(filters[0], 1)\n",
    "        # self.ds8 = DS_out(filters[1], 1)\n",
    "        # self.ds9 = DS_out(filters[2], 1)\n",
    "        self.ds10 = DS_out(filters[2], 1)\n",
    "    \n",
    "    def forward(self, # skip7, skip8, skip9):\n",
    "                skip9):\n",
    "        \n",
    "        # out7 = self.ds7(skip7)\n",
    "        # print(f\"DS 7 out -> {list(out7.size())}\")\n",
    "        # out8 = self.ds8(skip8)\n",
    "        # print(f\"DS 8 out -> {list(out8.size())}\")\n",
    "        # out9 = self.ds9(skip9)\n",
    "        # print(f\"DS 9 out -> {list(out9.size())}\")\n",
    "        out10 = self.ds10(skip9)\n",
    "        print(f\"DS 10 out -> {list(out10.size())}\")\n",
    "\n",
    "        return out10\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "#                                TAIL\n",
    "# =======================================================================\n",
    "\n",
    "model_tail = FCT_Tail()\n",
    "model_tail.apply(init_weights)\n",
    "\n",
    "optimizer_tail = torch.optim.AdamW(model_tail.parameters(), lr=dict_args['lr'],weight_decay=dict_args['decay'])\n",
    "\n",
    "scheduler_tail = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer_tail,\n",
    "            mode='min',\n",
    "            factor=dict_args['lr_factor'],\n",
    "            verbose=True,\n",
    "            threshold=1e-6,\n",
    "            patience=10,\n",
    "            min_lr=dict_args['min_lr'])\n",
    "\n",
    "model_tail.to(device)\n",
    "\n",
    "print(\"Initialized ....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation in Tail model\n",
    "\n",
    "model_tail.train()\n",
    "train_loss_list = []\n",
    "grads_dict = {}\n",
    "abs_grads_dict = {}\n",
    "\n",
    "\n",
    "body_fwd = h5py.File('params_and_grads/body_forward_pass.hdf5', 'r')\n",
    "train_label = h5py.File('params_and_grads/train_values.hdf5', 'r')\n",
    "\n",
    "try:\n",
    "    for (key, grp), (lkey, lgrp) in zip(body_fwd.items(), train_label.items()):\n",
    "\n",
    "        if str(key) != str(lkey):\n",
    "            print(f\"Not the same key tail:: {key} and label:: {lkey}, data could be different \")\n",
    "        \n",
    "        # skip_7 = torch.tensor(grp['skip7'][:], requires_grad=True).to(device)\n",
    "        # skip_8 = torch.tensor(grp['skip8'][:], requires_grad=True).to(device)\n",
    "        skip_9 = torch.tensor(grp['skip9'][:], requires_grad=True).to(device)\n",
    "        \n",
    "        y_mask = torch.from_numpy(lgrp['tlabel'][:]).to(device)\n",
    "\n",
    "        tl_output_data = model_tail(# skip_7, skip_8, \n",
    "            skip_9\n",
    "            )\n",
    "        \n",
    "        loss = loss_fn(tl_output_data, y_mask)\n",
    "        train_loss_list.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer_tail.step()\n",
    "\n",
    "except Exception as ex:\n",
    "    import traceback\n",
    "    print(\"+=\" * 25)\n",
    "    print(\"Error encountered as :\", ex)\n",
    "    print(\"+=\" * 25)\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    body_fwd.close()\n",
    "    train_label.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_dict = {}\n",
    "mean_grads_dict = {}\n",
    "\n",
    "for name, params in model_tail.named_parameters():\n",
    "    if (name not in grads_dict) and (\"ds10\" in name):\n",
    "        grads_dict[name] = []\n",
    "        mean_grads_dict[name] = []\n",
    "    if params.grad is not None:\n",
    "        grads_dict[name].append(params.grad)\n",
    "        mean_grads_dict[name].append(params.grad.shape)\n",
    "\n",
    "\n",
    "print(\"grads_dict : \\n\", grads_dict)\n",
    "print(\"=+\" * 15)\n",
    "print(\"abs_grads_dict : \\n\", mean_grads_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with h5py.File('params_and_grads/tail_back_prop.hdf5', 'w') as tail_grad:\n",
    "        for name, params in model_tail.named_parameters():\n",
    "            if params.requires_grad:\n",
    "                if \"ds10.conv1.0\" in name:\n",
    "                    tail_grad.create_dataset(name, data=params.grad)\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    fh5_body.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, params in model_tail.named_parameters():\n",
    "    if params.requires_grad:\n",
    "        if \"ds10.conv1.0\" in name:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
