{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinu/anaconda3/envs/mlenv/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libjpeg.so.8: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from model import Block_encoder_bottleneck, Block_decoder, device, dict_args, init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FCT_Body(nn.Module):\n",
    "    def __init__(self, ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        b_attent_body = [2, 2, 2, 2, 2]\n",
    "        filters = [64, 128, 64, 32, 16, 8] \n",
    "        # number of blocks used in the model\n",
    "        blocks = len(b_attent_body)\n",
    "\n",
    "        stochastic_depth_rate = 0.0\n",
    "\n",
    "        #probability for each block\n",
    "        dpr = [x for x in np.linspace(0, stochastic_depth_rate, blocks)]\n",
    "\n",
    "\n",
    "        # model\n",
    "        self.block_5 = Block_encoder_bottleneck(\"bottleneck\", filters[0], filters[1], b_attent_body[0], dpr[0])\n",
    "        self.block_6 = Block_decoder(filters[1], filters[2], b_attent_body[1], dpr[1])\n",
    "        self.block_7 = Block_decoder(filters[2], filters[3], b_attent_body[2], dpr[2])\n",
    "        self.block_8 = Block_decoder(filters[3], filters[4], b_attent_body[3], dpr[3])\n",
    "        self.block_9 = Block_decoder(filters[4], filters[5], b_attent_body[4], dpr[4])\n",
    "    \n",
    "    def forward(self, skip1, skip2, skip3, skip4):\n",
    "        \n",
    "        x = self.block_5(skip4)\n",
    "        print(f\"Block 5 out -> {list(x.size())}\")\n",
    "        x = self.block_6(x, skip4)\n",
    "        print(f\"Block 6 out -> {list(x.size())}\")\n",
    "        x = self.block_7(x, skip3)\n",
    "        print(f\"Block 7 out -> {list(x.size())}\")\n",
    "        skip7 = x\n",
    "        x = self.block_8(x, skip2)\n",
    "        print(f\"Block 8 out -> {list(x.size())}\")\n",
    "        skip8 = x\n",
    "        x = self.block_9(x, skip1)\n",
    "        print(f\"Block 9 out -> {list(x.size())}\")\n",
    "        skip9 = x\n",
    "\n",
    "        return {\n",
    "        #     \"skip7\": skip7.cpu().detach().numpy(), \n",
    "        #     \"skip8\": skip8.cpu().detach().numpy(), \n",
    "            \"skip9\": skip9.cpu().detach().numpy(),\n",
    "           }\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCT_Body(\n",
       "  (block_5): Block_encoder_bottleneck(\n",
       "    (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (trans): Transformer(\n",
       "      (attention_output): Attention(\n",
       "        (conv_q): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=128)\n",
       "        (layernorm_q): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_k): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "        (layernorm_k): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "        (layernorm_v): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (wide_focus): Wide_Focus(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2))\n",
       "        (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(3, 3))\n",
       "        (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_6): Block_decoder(\n",
       "    (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (trans): Transformer(\n",
       "      (attention_output): Attention(\n",
       "        (conv_q): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64)\n",
       "        (layernorm_q): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_k): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "        (layernorm_k): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "        (layernorm_v): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (wide_focus): Wide_Focus(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2))\n",
       "        (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(3, 3))\n",
       "        (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_7): Block_decoder(\n",
       "    (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (trans): Transformer(\n",
       "      (attention_output): Attention(\n",
       "        (conv_q): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=32)\n",
       "        (layernorm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_k): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "        (layernorm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_v): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "        (layernorm_v): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (layernorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (wide_focus): Wide_Focus(\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2))\n",
       "        (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(3, 3))\n",
       "        (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_8): Block_decoder(\n",
       "    (layernorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (trans): Transformer(\n",
       "      (attention_output): Attention(\n",
       "        (conv_q): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=16)\n",
       "        (layernorm_q): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_k): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)\n",
       "        (layernorm_k): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_v): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)\n",
       "        (layernorm_v): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (layernorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (wide_focus): Wide_Focus(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2))\n",
       "        (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(3, 3))\n",
       "        (conv4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_9): Block_decoder(\n",
       "    (layernorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (conv1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (trans): Transformer(\n",
       "      (attention_output): Attention(\n",
       "        (conv_q): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=8)\n",
       "        (layernorm_q): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_k): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "        (layernorm_k): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_v): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "        (layernorm_v): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (layernorm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (wide_focus): Wide_Focus(\n",
       "        (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2))\n",
       "        (conv3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(3, 3))\n",
       "        (conv4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================================================================\n",
    "#                                BODY\n",
    "# =======================================================================\n",
    "\n",
    "model_body = FCT_Body()\n",
    "model_body.apply(init_weights)\n",
    "\n",
    "optimizer_body = torch.optim.AdamW(model_body.parameters(), lr=dict_args['lr'],weight_decay=dict_args['decay'])\n",
    "\n",
    "scheduler_body = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer_body,\n",
    "            mode='min',\n",
    "            factor=dict_args['lr_factor'],\n",
    "            verbose=True,\n",
    "            threshold=1e-6,\n",
    "            patience=10,\n",
    "            min_lr=dict_args['min_lr'])\n",
    "\n",
    "model_body.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to synchronously create file (unable to truncate a file which is already open)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Forward propagation in body model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model_body\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> 5\u001b[0m body_fwd \u001b[39m=\u001b[39m h5py\u001b[39m.\u001b[39;49mFile(\u001b[39m'\u001b[39;49m\u001b[39mparams_and_grads/body_forward_pass.hdf5\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(\u001b[39m'\u001b[39m\u001b[39mparams_and_grads/head_forward_pass.hdf5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m head_fwd:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.9/site-packages/h5py/_hl/files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    558\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    560\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[1;32m    561\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[1;32m    562\u001b[0m                      meta_block_size\u001b[39m=\u001b[39mmeta_block_size,\n\u001b[1;32m    563\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    564\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[1;32m    565\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[1;32m    566\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 567\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    570\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.9/site-packages/h5py/_hl/files.py:237\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    235\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mcreate(name, h5f\u001b[39m.\u001b[39mACC_EXCL, fapl\u001b[39m=\u001b[39mfapl, fcpl\u001b[39m=\u001b[39mfcpl)\n\u001b[1;32m    236\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 237\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mcreate(name, h5f\u001b[39m.\u001b[39;49mACC_TRUNC, fapl\u001b[39m=\u001b[39;49mfapl, fcpl\u001b[39m=\u001b[39;49mfcpl)\n\u001b[1;32m    238\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    239\u001b[0m     \u001b[39m# Open in append mode (read/write).\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[39m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[39m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:126\u001b[0m, in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to synchronously create file (unable to truncate a file which is already open)"
     ]
    }
   ],
   "source": [
    "# Forward propagation in body model\n",
    "\n",
    "model_body.train()\n",
    "\n",
    "body_fwd = h5py.File('params_and_grads/body_forward_pass.hdf5', 'w')\n",
    "\n",
    "try:\n",
    "    with h5py.File('params_and_grads/head_forward_pass.hdf5', 'r') as head_fwd:\n",
    "        for key, grp in tqdm(head_fwd.items(), total=len(head_fwd)):\n",
    "            skip_1 = torch.from_numpy(grp['skip1'][:]).to(device)\n",
    "            skip_2 = torch.from_numpy(grp['skip2'][:]).to(device)\n",
    "            skip_3 = torch.from_numpy(grp['skip3'][:]).to(device)     \n",
    "            skip_4 = torch.from_numpy(grp['skip4'][:]).to(device)\n",
    "\n",
    "            bd_layer_data = model_body(skip_1, skip_2, skip_3, skip_4)\n",
    "\n",
    "            bgrp = body_fwd.create_group(key)\n",
    "            for k,v in bd_layer_data.items():\n",
    "                bgrp.create_dataset(k, data=v)\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    body_fwd.close()\n",
    "    head_fwd.close()\n",
    "finally:\n",
    "    body_fwd.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
