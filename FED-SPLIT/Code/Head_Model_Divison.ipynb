{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from monai import data, transforms as mt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from model import Block_encoder_bottleneck, device, dict_args, init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test data path\n",
    "\n",
    "train_image_path = \"../dataset/train_rosfl\"\n",
    "test_image_path = \"../dataset/testing\"\n",
    "\n",
    "# target/crop shape for the images and masks when training\n",
    "tar_shape = (256, 256)\n",
    "crop_shape = (224, 224)\n",
    "\n",
    "\n",
    "def normalize(data):\n",
    "    data = (data - data.mean()) / data.std()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCT_Head(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        h_attent_head = [2, 2, 2, 2, 2]\n",
    "        filters = [8, 16, 32, 64, 128,]\n",
    "        # number of blocks used in the model\n",
    "        blocks = len(filters)\n",
    "\n",
    "        stochastic_depth_rate = 0.0\n",
    "\n",
    "        #probability for each block\n",
    "        dpr = [x for x in np.linspace(0, stochastic_depth_rate, blocks)]\n",
    "\n",
    "        # Multi-scale input\n",
    "        self.scale_img = nn.AvgPool2d(2,2)   \n",
    "\n",
    "        # model\n",
    "        self.block_1 = Block_encoder_bottleneck(\"first\", 1, filters[0], h_attent_head[0], dpr[0])\n",
    "        self.block_2 = Block_encoder_bottleneck(\"second\", filters[0], filters[1], h_attent_head[1], dpr[1])\n",
    "        self.block_3 = Block_encoder_bottleneck(\"third\", filters[1], filters[2], h_attent_head[2], dpr[2])\n",
    "        self.block_4 = Block_encoder_bottleneck(\"fourth\", filters[2], filters[3], h_attent_head[3], dpr[3])\n",
    "        self.block_5 = Block_encoder_bottleneck(\"bottleneck\", filters[3], filters[4], h_attent_head[4], dpr[4])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Multi-scale input\n",
    "        scale_img_2 = self.scale_img(x)\n",
    "        scale_img_3 = self.scale_img(scale_img_2)\n",
    "        scale_img_4 = self.scale_img(scale_img_3)  \n",
    "\n",
    "        x = self.block_1(x)\n",
    "        print(f\"Block 1 out -> {list(x.size())}\")\n",
    "        skip1 = x\n",
    "        x = self.block_2(x, scale_img_2)\n",
    "        print(f\"Block 2 out -> {list(x.size())}\")\n",
    "        skip2 = x\n",
    "        x = self.block_3(x, scale_img_3)\n",
    "        print(f\"Block 3 out -> {list(x.size())}\")\n",
    "        skip3 = x\n",
    "        x = self.block_4(x, scale_img_4)\n",
    "        print(f\"Block 4 out -> {list(x.size())}\")\n",
    "        skip4 = x\n",
    "\n",
    "        return {\n",
    "            \"skip1\": skip1.cpu().detach().numpy(), \n",
    "            \"skip2\": skip2.cpu().detach().numpy(), \n",
    "            \"skip3\": skip3.cpu().detach().numpy(), \n",
    "            \"skip4\": skip4.cpu().detach().numpy(),\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACDC_Load_Gray(Dataset):\n",
    "    def __init__(self, source, ind, Transform=None):\n",
    "        # basic transforms\n",
    "        self.loader = mt.LoadImaged(keys=[\"image\", \"mask\"])\n",
    "        self.add_channel = mt.EnsureChannelFirstd(keys=[\"image\", \"mask\"])\n",
    "        self.spatial_pad = mt.SpatialPadD(keys=[\"image\", \"mask\"], spatial_size=tar_shape, mode=\"edge\")\n",
    "        self.spacing = mt.Spacingd(keys=[\"image\", \"mask\"], pixdim=(1.25, 1.25, -1.0), mode=(\"nearest\", \"nearest\"))\n",
    "        # index\n",
    "        self.ind = ind\n",
    "        # transform\n",
    "        if Transform is not None:\n",
    "            self.transform = Transform\n",
    "        else:\n",
    "            self.transform = mt.Compose([\n",
    "                mt.SpatialPadD(keys=[\"image\", \"mask\"], spatial_size=tar_shape, mode=\"edge\"),\n",
    "                mt.ToTensorD(keys=[\"image\", \"mask\"], allow_missing_keys=False)\n",
    "            ])\n",
    "\n",
    "        # take the images\n",
    "        source = Path(source)\n",
    "        # dirs = os.listdir(str(source))  # stores patient name\n",
    "        all_data_ed = []\n",
    "        all_data_ed_mask = []\n",
    "        all_data_es = []\n",
    "        all_data_es_mask = []\n",
    "        for filenames in source.iterdir():\n",
    "            if filenames.is_dir():\n",
    "                # patient_path = Path(str(source), filenames)  # individual patient path\n",
    "                patient_info = str(filenames / \"Info.cfg\")  # patient information\n",
    "                file = open(patient_info, 'r').readlines()\n",
    "                ED_frame = int(file[0].split(\":\")[1])\n",
    "                ES_frame = int(file[1].split(\":\")[1])\n",
    "                ED = (filenames / f\"{filenames.name}_frame{ED_frame:02d}.nii.gz\")\n",
    "                ES = (filenames / f\"{filenames.name}_frame{ES_frame:02d}.nii.gz\")\n",
    "                ED_gt = (filenames / f\"{filenames.name}_frame{ED_frame:02d}_gt.nii.gz\")\n",
    "                ES_gt = (filenames / f\"{filenames.name}_frame{ES_frame:02d}_gt.nii.gz\")\n",
    "                all_data_ed.append(ED)\n",
    "                all_data_ed_mask.append(ED_gt)\n",
    "                all_data_es.append(ES)\n",
    "                all_data_es_mask.append(ES_gt)\n",
    "\n",
    "        if self.ind is not None:\n",
    "            all_data_ed = [all_data_ed[i] for i in self.ind]\n",
    "            all_data_ed_mask = [all_data_ed_mask[i] for i in self.ind]\n",
    "            all_data_es = [all_data_es[i] for i in self.ind]\n",
    "            all_data_es_mask = [all_data_es_mask[i] for i in self.ind]\n",
    "\n",
    "        self.data = [all_data_ed, all_data_ed_mask, all_data_es, all_data_es_mask]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ED_img, ED_mask, ES_img, ES_mask = self.data[0][idx], self.data[1][idx], self.data[2][idx], self.data[3][idx]\n",
    "        # data dict\n",
    "        ED_data_dict = {\"image\": ED_img,\n",
    "                        \"mask\": ED_mask}\n",
    "        ES_data_dict = {\"image\": ES_img,\n",
    "                        \"mask\": ES_mask}\n",
    "        # instead of returning both ED and ES, I have to return just a random choice between ED and ES(image and mask)\n",
    "        datalist = [ED_data_dict, ES_data_dict]\n",
    "        data_return = np.random.choice(datalist)\n",
    "        data_return = self.loader(data_return)\n",
    "        data_return = self.add_channel(data_return)\n",
    "        data_return = self.spacing(data_return)\n",
    "        data_return[\"image\"] = normalize(data_return[\"image\"])\n",
    "        num_slice = data_return[\"image\"].shape[3]\n",
    "        random_slice = random.randint(0, num_slice - 1)\n",
    "        data_return[\"image\"] = data_return[\"image\"][:, :, :, random_slice]\n",
    "        data_return[\"image\"] = normalize(data_return[\"image\"])\n",
    "        data_return[\"mask\"] = data_return[\"mask\"][:, :, :, random_slice]\n",
    "        data_return = self.transform(data_return)\n",
    "        return data_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loader_ACDC(train_index, data_path=train_image_path, transform=None):\n",
    "    train_loader = ACDC_Load_Gray(source=data_path, Transform=transform, ind=train_index)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def val_loader_ACDC(val_index, data_path=train_image_path, transform=None):\n",
    "    val_loader = ACDC_Load_Gray(source=data_path, Transform=transform, ind=val_index)\n",
    "    return val_loader\n",
    "\n",
    "\n",
    "def test_loader_ACDC(test_index, data_path=test_image_path, transform=None):\n",
    "    test_loader = ACDC_Load_Gray(source=data_path, Transform=transform, ind=test_index)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" To load the training and validation dataloader works \"\"\"\n",
    "\n",
    "train_compose = mt.Compose(\n",
    "    [mt.SpatialPadD(keys=[\"image\", \"mask\"], spatial_size=tar_shape, mode=\"edge\"),\n",
    "     mt.RandSpatialCropD(keys=[\"image\", \"mask\"], roi_size=crop_shape, random_center=True, random_size=False),\n",
    "     mt.ToTensorD(keys=[\"image\", \"mask\"], allow_missing_keys=False),\n",
    "     ]\n",
    ")\n",
    "\n",
    "val_compose = mt.Compose(\n",
    "    [   mt.SpatialPadD(keys=[\"image\", \"mask\"], spatial_size=tar_shape, mode=\"edge\"),\n",
    "        mt.RandSpatialCropD(keys=[\"image\", \"mask\"], roi_size=crop_shape, random_center=True, random_size=False),\n",
    "        mt.ToTensorD(keys=[\"image\", \"mask\"], allow_missing_keys=False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "splits = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "concatenated_dataset = train_loader_ACDC(transform=None, train_index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" To load the testing dataloader works \"\"\"\n",
    "test_compose = mt.Compose(\n",
    "    [\n",
    "        mt.SpatialPadD(keys=[\"image\", \"mask\"], spatial_size=tar_shape, mode=\"edge\"),\n",
    "        mt.RandSpatialCropD(keys=[\"image\", \"mask\"], roi_size=crop_shape, random_center=True, random_size=False),\n",
    "        mt.ToTensorD(keys=[\"image\", \"mask\"], allow_missing_keys=False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_data = DataLoader(test_loader_ACDC(transform=test_compose, test_index=None), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Fold 1 --------------------------\n",
      "train from here 20\n",
      "val from here 20\n",
      "-------------------------- Fold 2 --------------------------\n",
      "train from here 20\n",
      "val from here 20\n",
      "-------------------------- Fold 3 --------------------------\n",
      "train from here 20\n",
      "val from here 20\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(concatenated_dataset)))):\n",
    "\n",
    "    print(\"--------------------------\", \"Fold\", fold + 1, \"--------------------------\")\n",
    "\n",
    "    # training dataset\n",
    "    training_data = DataLoader(train_loader_ACDC(transform=train_compose, train_index=train_idx), batch_size=2,\n",
    "                               shuffle=False)\n",
    "    print(\"train from here\", len(training_data))\n",
    "    # for dic in training_data:\n",
    "    #     images = dic[\"image\"]\n",
    "    #     masks = dic[\"mask\"]\n",
    "    #     print(images.shape, masks.shape)\n",
    "    #     image, label = dic[\"image\"], dic[\"mask\"]\n",
    "    #     plt.figure(\"visualise\", (8, 4))\n",
    "    #     plt.subplot(1, 2, 1)\n",
    "    #     plt.title(\"image\")\n",
    "    #     plt.imshow(image[0, 0, :, :], cmap=\"gray\")\n",
    "    #     plt.subplot(1, 2, 2)\n",
    "    #     plt.title(\"mask\")\n",
    "    #     plt.imshow(label[0, 0, :, :], cmap=\"gray\")\n",
    "    #     plt.show()\n",
    "    #     break\n",
    "\n",
    "    # validation dataset\n",
    "    validation_data = DataLoader(val_loader_ACDC(transform=val_compose, val_index=val_idx), batch_size=1,\n",
    "                                 shuffle=False)\n",
    "    print(\"val from here\", len(validation_data))\n",
    "    # for dic in validation_data:\n",
    "    #     images = dic[\"image\"]\n",
    "    #     masks = dic[\"mask\"]\n",
    "    #     print(images.shape, masks.shape)\n",
    "    #     image, label = dic[\"image\"], dic[\"mask\"]\n",
    "    #     plt.figure(\"visualise\", (8, 4))\n",
    "    #     plt.subplot(1, 2, 1)\n",
    "    #     plt.title(\"image\")\n",
    "    #     plt.imshow(image[0, 0, :, :], cmap=\"gray\")\n",
    "    #     plt.subplot(1, 2, 2)\n",
    "    #     plt.title(\"mask\")\n",
    "    #     plt.imshow(label[0, 0, :, :], cmap=\"gray\")\n",
    "    #     plt.show()\n",
    "    #     break\n",
    "\n",
    "    # test dataset\n",
    "    # ========================== TEST Data ===================\n",
    "    # test_data = DataLoader(test_loader_ACDC(transform=test_compose, test_index=None), batch_size=1, shuffle=False)\n",
    "    # ========================== TEST Data ===================\n",
    "    # print(\"test from here\")\n",
    "    # for dic in test_data:\n",
    "    #     images = dic[\"image\"]\n",
    "    #     masks = dic[\"mask\"]\n",
    "    #     print(images.shape, masks.shape)\n",
    "    #     image, label = dic[\"image\"], dic[\"mask\"]\n",
    "    #     plt.figure(\"visualise\", (8, 4))\n",
    "    #     plt.subplot(1, 2, 1)\n",
    "    #     plt.title(\"image\")\n",
    "    #     plt.imshow(image[0, 0, :, :], cmap=\"gray\")\n",
    "    #     plt.subplot(1, 2, 2)\n",
    "    #     plt.title(\"mask\")\n",
    "    #     plt.imshow(label[0, 0, :, :], cmap=\"gray\")\n",
    "    #     plt.show()\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCT_Head(\n",
       "  (scale_img): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (block_1): Block_encoder_bottleneck(\n",
       "    (layernorm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "    (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (trans): Transformer(\n",
       "      (attention_output): Attention(\n",
       "        (conv_q): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=8)\n",
       "        (layernorm_q): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_k): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "        (layernorm_k): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_v): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
       "        (layernorm_v): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (layernorm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (wide_focus): Wide_Focus(\n",
       "        (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2))\n",
       "        (conv3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(3, 3))\n",
       "        (conv4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_2): Block_encoder_bottleneck(\n",
       "    (layernorm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (trans): Transformer(\n",
       "      (attention_output): Attention(\n",
       "        (conv_q): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=16)\n",
       "        (layernorm_q): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_k): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)\n",
       "        (layernorm_k): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_v): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)\n",
       "        (layernorm_v): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (layernorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (wide_focus): Wide_Focus(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2))\n",
       "        (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(3, 3))\n",
       "        (conv4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_3): Block_encoder_bottleneck(\n",
       "    (layernorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (trans): Transformer(\n",
       "      (attention_output): Attention(\n",
       "        (conv_q): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=32)\n",
       "        (layernorm_q): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_k): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "        (layernorm_k): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_v): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "        (layernorm_v): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (layernorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (wide_focus): Wide_Focus(\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2))\n",
       "        (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(3, 3))\n",
       "        (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_4): Block_encoder_bottleneck(\n",
       "    (layernorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (trans): Transformer(\n",
       "      (attention_output): Attention(\n",
       "        (conv_q): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64)\n",
       "        (layernorm_q): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_k): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "        (layernorm_k): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "        (layernorm_v): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (wide_focus): Wide_Focus(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2))\n",
       "        (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(3, 3))\n",
       "        (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block_5): Block_encoder_bottleneck(\n",
       "    (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (trans): Transformer(\n",
       "      (attention_output): Attention(\n",
       "        (conv_q): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=128)\n",
       "        (layernorm_q): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_k): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "        (layernorm_k): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "        (layernorm_v): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (wide_focus): Wide_Focus(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2))\n",
       "        (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(3, 3))\n",
       "        (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================================================================\n",
    "#                                HEAD\n",
    "# =======================================================================\n",
    "\n",
    "model_head = FCT_Head()\n",
    "model_head.apply(init_weights)\n",
    "\n",
    "optimizer_head = torch.optim.Adam(model_head.parameters(), lr=dict_args['lr'],weight_decay=dict_args['decay'])\n",
    "\n",
    "scheduler_head = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer_head,\n",
    "            mode='min',\n",
    "            factor=dict_args['lr_factor'],\n",
    "            verbose=True,\n",
    "            threshold=1e-6,\n",
    "            patience=10,\n",
    "            min_lr=dict_args['min_lr'])\n",
    "\n",
    "model_head.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index value is  0\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  1\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  2\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  3\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  4\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  5\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  6\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  7\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  8\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  9\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  10\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  11\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  12\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  13\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  14\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  15\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  16\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  17\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  18\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n",
      "index value is  19\n",
      "Block 1 out -> [2, 8, 112, 112]\n",
      "Block 2 out -> [2, 16, 56, 56]\n",
      "Block 3 out -> [2, 32, 28, 28]\n",
      "Block 4 out -> [2, 64, 14, 14]\n"
     ]
    }
   ],
   "source": [
    "model_head.train()\n",
    "\n",
    "try:\n",
    "    head_fwd = h5py.File('params_and_grads/head_forward_pass.hdf5', 'w') \n",
    "    train_label = h5py.File('params_and_grads/train_values.hdf5', 'w')\n",
    "    for index, train_dict in enumerate(training_data):\n",
    "        print(\"index value is \", index)\n",
    "        X_train = train_dict[\"image\"]\n",
    "        y_train = train_dict[\"mask\"]\n",
    "        X_train = X_train.to(device)\n",
    "\n",
    "        layer_data = model_head(X_train)\n",
    "\n",
    "        grp_head = head_fwd.create_group(f'IterKey_{index}')\n",
    "        for k, v in layer_data.items():\n",
    "            grp_head.create_dataset(k, data=v)\n",
    "        \n",
    "        grp_label = train_label.create_group(f'IterKey_{index}')\n",
    "        grp_label.create_dataset(\"tlabel\", data=y_train.cpu().detach().numpy())\n",
    "except Exception as ex:\n",
    "    import traceback\n",
    "    print(\"+=\" * 25)\n",
    "    print(\"Error encountered as :\", ex)\n",
    "    print(\"+=\" * 25)\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    head_fwd.close()\n",
    "    train_label.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
